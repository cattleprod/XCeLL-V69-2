From aecd049b37d66e0e1145fe604ce4aa0d4d498343 Mon Sep 17 00:00:00 2001
Message-Id: <aecd049b37d66e0e1145fe604ce4aa0d4d498343.1315184977.git.hot2009wheels@hotmail.com>
In-Reply-To: <a4656eef4910f3ef802417e6bd17e3983b2fd5db.1315184977.git.hot2009wheels@hotmail.com>
References: <a4656eef4910f3ef802417e6bd17e3983b2fd5db.1315184977.git.hot2009wheels@hotmail.com>
From: LeeDroid- <leedroid.lb@gmail.com>
Date: Tue, 16 Aug 2011 00:15:47 +0100
Subject: [PATCH 14/14] Add: Min_Max & SmartAss Governors (Smartass fixes thanks to faux123)

---
 drivers/cpufreq/Kconfig            |   47 +++
 drivers/cpufreq/Makefile           |    2 +
 drivers/cpufreq/cpufreq_minmax.c   |  573 ++++++++++++++++++++++++++++++
 drivers/cpufreq/cpufreq_smartass.c |  682 ++++++++++++++++++++++++++++++++++++
 include/linux/cpufreq.h            |    6 +
 5 files changed, 1310 insertions(+), 0 deletions(-)

diff --git a/drivers/cpufreq/Kconfig b/drivers/cpufreq/Kconfig
index cd3464b..0de7721 100644
--- a/drivers/cpufreq/Kconfig
+++ b/drivers/cpufreq/Kconfig
@@ -111,6 +111,22 @@ config CPU_FREQ_DEFAULT_GOV_CONSERVATIVE
 	  governor. If unsure have a look at the help section of the
 	  driver. Fallback governor will be the performance governor.
 
+config CPU_FREQ_DEFAULT_GOV_MINMAX
+	bool "minmax"
+	select CPU_FREQ_GOV_MINMAX
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'minmax' as default. This minimizes the
+	  frequency jumps does by the governor. This is aimed at maximizing
+	  both perfomance and battery life.
+
+config CPU_FREQ_DEFAULT_GOV_SMARTASS
+	bool "smartass"
+	select CPU_FREQ_GOV_SMARTASS
+	select CPU_FREQ_GOV_PERFORMANCE
+	help
+	  Use the CPUFreq governor 'smartass' as default.
+
 config CPU_FREQ_DEFAULT_GOV_INTERACTIVE
 	bool "interactive"
 	select CPU_FREQ_GOV_INTERACTIVE
@@ -206,4 +222,35 @@ config CPU_FREQ_GOV_CONSERVATIVE
 
 	  If in doubt, say N.
 
+config CPU_FREQ_GOV_MINMAX
+	tristate "'minmax' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  'minmax' - this driver tries to minimize the frequency jumps by limiting
+	  the the selected frequencies to either the min or the max frequency of
+	  the policy. The frequency is selected according to the load.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_GOV_SMARTASS
+	tristate "'smartass' cpufreq governor"
+	depends on CPU_FREQ
+	help
+	  'smartass' - a "smart" optimized governor for the hero!
+
+	  If in doubt, say N.
+
+config CPU_FREQ_MIN_TICKS
+	int "Ticks between governor polling interval."
+	default 10
+	help
+	  Minimum number of ticks between polling interval for governors.
+
+config CPU_FREQ_SAMPLING_LATENCY_MULTIPLIER
+	int "Sampling rate multiplier for governors."
+	default 1000
+	help
+	  Sampling latency rate multiplied by the cpu switch latency.
+	  Affects governor polling.
+
 endif	# CPU_FREQ
diff --git a/drivers/cpufreq/Makefile b/drivers/cpufreq/Makefile
index 30629f7..f043fc7 100644
--- a/drivers/cpufreq/Makefile
+++ b/drivers/cpufreq/Makefile
@@ -9,6 +9,8 @@ obj-$(CONFIG_CPU_FREQ_GOV_POWERSAVE)	+= cpufreq_powersave.o
 obj-$(CONFIG_CPU_FREQ_GOV_USERSPACE)	+= cpufreq_userspace.o
 obj-$(CONFIG_CPU_FREQ_GOV_ONDEMAND)	+= cpufreq_ondemand.o
 obj-$(CONFIG_CPU_FREQ_GOV_CONSERVATIVE)	+= cpufreq_conservative.o
+obj-$(CONFIG_CPU_FREQ_GOV_MINMAX)	+= cpufreq_minmax.o
+obj-$(CONFIG_CPU_FREQ_GOV_SMARTASS)	+= cpufreq_smartass.o
 obj-$(CONFIG_CPU_FREQ_GOV_INTERACTIVE)	+= cpufreq_interactive.o
 
 # CPUfreq cross-arch helpers
diff --git a/drivers/cpufreq/cpufreq_minmax.c b/drivers/cpufreq/cpufreq_minmax.c
new file mode 100644
index 0000000..98b45f0
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_minmax.c
@@ -0,0 +1,573 @@
+/*
+ *  drivers/cpufreq/cpufreq_minmax.c
+ *
+ *  Copyright (C)  2001 Russell King
+ *            (C)  2003 Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>.
+ *                      Jun Nakajima <jun.nakajima@intel.com>
+ *            (C)  2004 Alexander Clouter <alex-kernel@digriz.org.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This governor is an adapatation of the conservative governor.
+ * See the Documentation/cpu-freq/governors.txt for more information.
+ *
+ * Adapatation from conservative by Erasmux.
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/smp.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/ctype.h>
+#include <linux/cpufreq.h>
+#include <linux/sysctl.h>
+#include <linux/types.h>
+#include <linux/fs.h>
+#include <linux/sysfs.h>
+#include <linux/cpu.h>
+#include <linux/kmod.h>
+#include <linux/workqueue.h>
+#include <linux/jiffies.h>
+#include <linux/kernel_stat.h>
+#include <linux/percpu.h>
+#include <linux/mutex.h>
+
+/*
+ * dbs is used in this file as a shortform for demandbased switching
+ * It helps to keep variable names smaller, simpler
+ */
+
+#define DEF_FREQUENCY_UP_THRESHOLD		(92)
+#define DEF_FREQUENCY_DOWN_THRESHOLD		(27)
+
+/*
+ * The polling frequency of this governor depends on the capability of
+ * the processor. Default polling frequency is 1000 times the transition
+ * latency of the processor. The governor will work on any processor with
+ * transition latency <= 10mS, using appropriate sampling
+ * rate.
+ * For CPUs with transition latency > 10mS (mostly drivers
+ * with CPUFREQ_ETERNAL), this governor will not work.
+ * All times here are in uS.
+ */
+static unsigned int def_sampling_rate;
+#define MIN_SAMPLING_RATE_RATIO			(2)
+/* for correct statistics, we need at least 10 ticks between each measure */
+#define MIN_STAT_SAMPLING_RATE			\
+	(MIN_SAMPLING_RATE_RATIO * jiffies_to_usecs(CONFIG_CPU_FREQ_MIN_TICKS))
+#define MIN_SAMPLING_RATE			\
+			(def_sampling_rate / MIN_SAMPLING_RATE_RATIO)
+#define MAX_SAMPLING_RATE			(500 * def_sampling_rate)
+#define DEF_SAMPLING_DOWN_FACTOR		(10)
+#define MAX_SAMPLING_DOWN_FACTOR		(100)
+#define TRANSITION_LATENCY_LIMIT		(10 * 1000 * 1000)
+
+static void do_dbs_timer(struct work_struct *work);
+
+struct cpu_dbs_info_s {
+	struct cpufreq_policy *cur_policy;
+	unsigned int prev_cpu_idle_up;
+	unsigned int prev_cpu_idle_down;
+	unsigned int enable;
+	unsigned int down_skip;
+	unsigned int requested_freq;
+};
+static DEFINE_PER_CPU(struct cpu_dbs_info_s, cpu_dbs_info);
+
+static unsigned int dbs_enable;	/* number of CPUs using this policy */
+
+/*
+ * DEADLOCK ALERT! There is a ordering requirement between cpu_hotplug
+ * lock and dbs_mutex. cpu_hotplug lock should always be held before
+ * dbs_mutex. If any function that can potentially take cpu_hotplug lock
+ * (like __cpufreq_driver_target()) is being called with dbs_mutex taken, then
+ * cpu_hotplug lock should be taken before that. Note that cpu_hotplug lock
+ * is recursive for the same process. -Venki
+ */
+static DEFINE_MUTEX (dbs_mutex);
+static DECLARE_DELAYED_WORK(dbs_work, do_dbs_timer);
+
+struct dbs_tuners {
+	unsigned int sampling_rate;
+	unsigned int sampling_down_factor;
+	unsigned int up_threshold;
+	unsigned int down_threshold;
+	unsigned int ignore_nice;
+};
+
+static struct dbs_tuners dbs_tuners_ins = {
+	.up_threshold = DEF_FREQUENCY_UP_THRESHOLD,
+	.down_threshold = DEF_FREQUENCY_DOWN_THRESHOLD,
+	.sampling_down_factor = DEF_SAMPLING_DOWN_FACTOR,
+	.ignore_nice = 0,
+};
+
+static inline unsigned int get_cpu_idle_time(unsigned int cpu)
+{
+	unsigned int add_nice = 0, ret;
+
+	if (dbs_tuners_ins.ignore_nice)
+		add_nice = kstat_cpu(cpu).cpustat.nice;
+
+	ret = kstat_cpu(cpu).cpustat.idle +
+		kstat_cpu(cpu).cpustat.iowait +
+		add_nice;
+
+	return ret;
+}
+
+/* keep track of frequency transitions */
+static int
+dbs_cpufreq_notifier(struct notifier_block *nb, unsigned long val,
+		     void *data)
+{
+	struct cpufreq_freqs *freq = data;
+	struct cpu_dbs_info_s *this_dbs_info = &per_cpu(cpu_dbs_info,
+							freq->cpu);
+
+	if (!this_dbs_info->enable)
+		return 0;
+
+	this_dbs_info->requested_freq = freq->new;
+
+	return 0;
+}
+
+static struct notifier_block dbs_cpufreq_notifier_block = {
+	.notifier_call = dbs_cpufreq_notifier
+};
+
+/************************** sysfs interface ************************/
+static ssize_t show_sampling_rate_max(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf (buf, "%u\n", MAX_SAMPLING_RATE);
+}
+
+static ssize_t show_sampling_rate_min(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf (buf, "%u\n", MIN_SAMPLING_RATE);
+}
+
+#define define_one_ro(_name)				\
+static struct freq_attr _name =				\
+__ATTR(_name, 0444, show_##_name, NULL)
+
+define_one_ro(sampling_rate_max);
+define_one_ro(sampling_rate_min);
+
+/* cpufreq_minmax Governor Tunables */
+#define show_one(file_name, object)					\
+static ssize_t show_##file_name						\
+(struct cpufreq_policy *unused, char *buf)				\
+{									\
+	return sprintf(buf, "%u\n", dbs_tuners_ins.object);		\
+}
+show_one(sampling_rate, sampling_rate);
+show_one(sampling_down_factor, sampling_down_factor);
+show_one(up_threshold, up_threshold);
+show_one(down_threshold, down_threshold);
+show_one(ignore_nice_load, ignore_nice);
+
+static ssize_t store_sampling_down_factor(struct cpufreq_policy *unused,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf (buf, "%u", &input);
+	if (ret != 1 || input > MAX_SAMPLING_DOWN_FACTOR || input < 1)
+		return -EINVAL;
+
+	mutex_lock(&dbs_mutex);
+	dbs_tuners_ins.sampling_down_factor = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_sampling_rate(struct cpufreq_policy *unused,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf (buf, "%u", &input);
+
+	mutex_lock(&dbs_mutex);
+	if (ret != 1 || input > MAX_SAMPLING_RATE || input < MIN_SAMPLING_RATE) {
+		mutex_unlock(&dbs_mutex);
+		return -EINVAL;
+	}
+
+	dbs_tuners_ins.sampling_rate = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_up_threshold(struct cpufreq_policy *unused,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf (buf, "%u", &input);
+
+	mutex_lock(&dbs_mutex);
+	if (ret != 1 || input > 100 || input <= dbs_tuners_ins.down_threshold) {
+		mutex_unlock(&dbs_mutex);
+		return -EINVAL;
+	}
+
+	dbs_tuners_ins.up_threshold = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_down_threshold(struct cpufreq_policy *unused,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+	ret = sscanf (buf, "%u", &input);
+
+	mutex_lock(&dbs_mutex);
+	if (ret != 1 || input > 100 || input >= dbs_tuners_ins.up_threshold) {
+		mutex_unlock(&dbs_mutex);
+		return -EINVAL;
+	}
+
+	dbs_tuners_ins.down_threshold = input;
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+static ssize_t store_ignore_nice_load(struct cpufreq_policy *policy,
+		const char *buf, size_t count)
+{
+	unsigned int input;
+	int ret;
+
+	unsigned int j;
+
+	ret = sscanf(buf, "%u", &input);
+	if (ret != 1)
+		return -EINVAL;
+
+	if (input > 1)
+		input = 1;
+
+	mutex_lock(&dbs_mutex);
+	if (input == dbs_tuners_ins.ignore_nice) { /* nothing to do */
+		mutex_unlock(&dbs_mutex);
+		return count;
+	}
+	dbs_tuners_ins.ignore_nice = input;
+
+	/* we need to re-evaluate prev_cpu_idle_up and prev_cpu_idle_down */
+	for_each_online_cpu(j) {
+		struct cpu_dbs_info_s *j_dbs_info;
+		j_dbs_info = &per_cpu(cpu_dbs_info, j);
+		j_dbs_info->prev_cpu_idle_up = get_cpu_idle_time(j);
+		j_dbs_info->prev_cpu_idle_down = j_dbs_info->prev_cpu_idle_up;
+	}
+	mutex_unlock(&dbs_mutex);
+
+	return count;
+}
+
+#define define_one_rw(_name) \
+static struct freq_attr _name = \
+__ATTR(_name, 0644, show_##_name, store_##_name)
+
+define_one_rw(sampling_rate);
+define_one_rw(sampling_down_factor);
+define_one_rw(up_threshold);
+define_one_rw(down_threshold);
+define_one_rw(ignore_nice_load);
+
+static struct attribute * dbs_attributes[] = {
+	&sampling_rate_max.attr,
+	&sampling_rate_min.attr,
+	&sampling_rate.attr,
+	&sampling_down_factor.attr,
+	&up_threshold.attr,
+	&down_threshold.attr,
+	&ignore_nice_load.attr,
+	NULL
+};
+
+static struct attribute_group dbs_attr_group = {
+	.attrs = dbs_attributes,
+	.name = "minmax",
+};
+
+/************************** sysfs end ************************/
+
+static void dbs_check_cpu(int cpu)
+{
+	unsigned int idle_ticks, up_idle_ticks, down_idle_ticks;
+	unsigned int tmp_idle_ticks, total_idle_ticks;
+	//unsigned int freq_target;
+	unsigned int freq_down_sampling_rate;
+	struct cpu_dbs_info_s *this_dbs_info = &per_cpu(cpu_dbs_info, cpu);
+	struct cpufreq_policy *policy;
+
+	if (!this_dbs_info->enable)
+		return;
+
+	policy = this_dbs_info->cur_policy;
+
+	/*
+	 * The default safe range is 20% to 80%
+	 * Every sampling_rate, we check
+	 *	- If current idle time is less than 20%, then we try to
+	 *	  increase frequency
+	 * Every sampling_rate*sampling_down_factor, we check
+	 *	- If current idle time is more than 80%, then we try to
+	 *	  decrease frequency
+	 *
+	 */
+
+	this_dbs_info->down_skip++;
+
+	/* Check for frequency increase */
+	idle_ticks = UINT_MAX;
+
+	/* Check for frequency increase */
+	total_idle_ticks = get_cpu_idle_time(cpu);
+	tmp_idle_ticks = total_idle_ticks -
+		this_dbs_info->prev_cpu_idle_up;
+	this_dbs_info->prev_cpu_idle_up = total_idle_ticks;
+
+	if (tmp_idle_ticks < idle_ticks)
+		idle_ticks = tmp_idle_ticks;
+
+	/* Scale idle ticks by 100 and compare with up and down ticks */
+	idle_ticks *= 100;
+	up_idle_ticks = (100 - dbs_tuners_ins.up_threshold) *
+			usecs_to_jiffies(dbs_tuners_ins.sampling_rate);
+
+	if (idle_ticks < up_idle_ticks) {
+		this_dbs_info->down_skip = 0;
+		this_dbs_info->prev_cpu_idle_down =
+			this_dbs_info->prev_cpu_idle_up;
+
+		/* if we are already at full speed then break out early */
+		if (this_dbs_info->requested_freq == policy->max)
+			return;
+
+		this_dbs_info->requested_freq = policy->max;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+			CPUFREQ_RELATION_H);
+		return;
+	}
+
+	/* Check for frequency decrease */
+	if (this_dbs_info->down_skip < dbs_tuners_ins.sampling_down_factor)
+		return;
+	else this_dbs_info->down_skip--; /* just to prevent overflow */
+
+
+	/* Check for frequency decrease */
+	total_idle_ticks = this_dbs_info->prev_cpu_idle_up;
+	tmp_idle_ticks = total_idle_ticks -
+		this_dbs_info->prev_cpu_idle_down;
+	this_dbs_info->prev_cpu_idle_down = total_idle_ticks;
+
+	if (tmp_idle_ticks < idle_ticks)
+		idle_ticks = tmp_idle_ticks;
+
+	/* Scale idle ticks by 100 and compare with up and down ticks */
+	idle_ticks *= 100;
+
+	freq_down_sampling_rate = dbs_tuners_ins.sampling_rate *
+		dbs_tuners_ins.sampling_down_factor;
+	down_idle_ticks = (100 - dbs_tuners_ins.down_threshold) *
+		usecs_to_jiffies(freq_down_sampling_rate);
+
+	if (idle_ticks > down_idle_ticks) {
+		/*
+		 * if we are already at the lowest speed then break out early
+		 * or if we 'cannot' reduce the speed as the user might want
+		 * freq_target to be zero
+		 */
+		if (this_dbs_info->requested_freq == policy->min)
+			return;
+
+		this_dbs_info->requested_freq = policy->min;
+
+		__cpufreq_driver_target(policy, this_dbs_info->requested_freq,
+				CPUFREQ_RELATION_H);
+		return;
+	}
+}
+
+static void do_dbs_timer(struct work_struct *work)
+{
+	int i;
+
+	mutex_lock(&dbs_mutex);
+	for_each_online_cpu(i)
+		dbs_check_cpu(i);
+	schedule_delayed_work(&dbs_work,
+			usecs_to_jiffies(dbs_tuners_ins.sampling_rate));
+	mutex_unlock(&dbs_mutex);
+}
+
+static inline void dbs_timer_init(void)
+{
+	init_timer_deferrable(&dbs_work.timer);
+	schedule_delayed_work(&dbs_work,
+			usecs_to_jiffies(dbs_tuners_ins.sampling_rate));
+	return;
+}
+
+static inline void dbs_timer_exit(void)
+{
+	cancel_delayed_work(&dbs_work);
+	return;
+}
+
+static int cpufreq_governor_dbs(struct cpufreq_policy *policy,
+				   unsigned int event)
+{
+	unsigned int cpu = policy->cpu;
+	struct cpu_dbs_info_s *this_dbs_info;
+	unsigned int j;
+	int rc;
+
+	this_dbs_info = &per_cpu(cpu_dbs_info, cpu);
+
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!policy->cur))
+			return -EINVAL;
+
+		if (this_dbs_info->enable) /* Already enabled */
+			break;
+
+		mutex_lock(&dbs_mutex);
+
+		rc = sysfs_create_group(&policy->kobj, &dbs_attr_group);
+		if (rc) {
+			mutex_unlock(&dbs_mutex);
+			return rc;
+		}
+
+		for_each_cpu(j, policy->cpus) {
+			struct cpu_dbs_info_s *j_dbs_info;
+			j_dbs_info = &per_cpu(cpu_dbs_info, j);
+			j_dbs_info->cur_policy = policy;
+
+			j_dbs_info->prev_cpu_idle_up = get_cpu_idle_time(cpu);
+			j_dbs_info->prev_cpu_idle_down
+				= j_dbs_info->prev_cpu_idle_up;
+		}
+		this_dbs_info->enable = 1;
+		this_dbs_info->down_skip = 0;
+		this_dbs_info->requested_freq = policy->cur;
+
+		dbs_enable++;
+		/*
+		 * Start the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 1) {
+			unsigned int latency;
+			/* policy latency is in nS. Convert it to uS first */
+			latency = policy->cpuinfo.transition_latency / 1000;
+			if (latency == 0)
+				latency = 1;
+
+			def_sampling_rate = 10 * latency *
+				CONFIG_CPU_FREQ_SAMPLING_LATENCY_MULTIPLIER;
+
+			if (def_sampling_rate < MIN_STAT_SAMPLING_RATE)
+				def_sampling_rate = MIN_STAT_SAMPLING_RATE;
+
+			dbs_tuners_ins.sampling_rate = def_sampling_rate;
+
+			dbs_timer_init();
+			cpufreq_register_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+		}
+
+		mutex_unlock(&dbs_mutex);
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		mutex_lock(&dbs_mutex);
+		this_dbs_info->enable = 0;
+		sysfs_remove_group(&policy->kobj, &dbs_attr_group);
+		dbs_enable--;
+		/*
+		 * Stop the timerschedule work, when this governor
+		 * is used for first time
+		 */
+		if (dbs_enable == 0) {
+			dbs_timer_exit();
+			cpufreq_unregister_notifier(
+					&dbs_cpufreq_notifier_block,
+					CPUFREQ_TRANSITION_NOTIFIER);
+		}
+
+		mutex_unlock(&dbs_mutex);
+
+		break;
+
+	case CPUFREQ_GOV_LIMITS:
+		mutex_lock(&dbs_mutex);
+		if (policy->max < this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->max, CPUFREQ_RELATION_H);
+		else if (policy->min > this_dbs_info->cur_policy->cur)
+			__cpufreq_driver_target(
+					this_dbs_info->cur_policy,
+					policy->min, CPUFREQ_RELATION_L);
+		mutex_unlock(&dbs_mutex);
+		break;
+	}
+	return 0;
+}
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_MINMAX
+static
+#endif
+struct cpufreq_governor cpufreq_gov_minmax = {
+	.name			= "minmax",
+	.governor		= cpufreq_governor_dbs,
+	.max_transition_latency	= TRANSITION_LATENCY_LIMIT,
+	.owner			= THIS_MODULE,
+};
+
+static int __init cpufreq_gov_dbs_init(void)
+{
+	return cpufreq_register_governor(&cpufreq_gov_minmax);
+}
+
+static void __exit cpufreq_gov_dbs_exit(void)
+{
+	/* Make sure that the scheduled work is indeed not running */
+	flush_scheduled_work();
+
+	cpufreq_unregister_governor(&cpufreq_gov_minmax);
+}
+
+MODULE_AUTHOR ("Erasmux");
+MODULE_DESCRIPTION ("'cpufreq_minmax' - A dynamic cpufreq governor which "
+		"minimizes the frequecy jumps by always selecting either "
+		"the minimal or maximal frequency");
+MODULE_LICENSE ("GPL");
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_MINMAX
+fs_initcall(cpufreq_gov_dbs_init);
+#else
+module_init(cpufreq_gov_dbs_init);
+#endif
+module_exit(cpufreq_gov_dbs_exit);
diff --git a/drivers/cpufreq/cpufreq_smartass.c b/drivers/cpufreq/cpufreq_smartass.c
new file mode 100644
index 0000000..c0075c7
--- /dev/null
+++ b/drivers/cpufreq/cpufreq_smartass.c
@@ -0,0 +1,682 @@
+/*
+ * drivers/cpufreq/cpufreq_smartass.c
+ *
+ * Copyright (C) 2010 Google, Inc.
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Author: Erasmux
+ * Modded by: faux123 for SMP compatibility
+ *
+ * Based on the interactive governor By Mike Chan (mike@android.com)
+ * which was adaptated to 2.6.29 kernel by Nadlabak (pavel@doshaska.net)
+ * 
+ * requires to add
+ * EXPORT_SYMBOL_GPL(nr_running);
+ * at the end of kernel/sched.c
+ *
+ */
+
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/tick.h>
+#include <linux/timer.h>
+#include <linux/workqueue.h>
+#include <linux/moduleparam.h>
+#include <asm/cputime.h>
+#include <linux/earlysuspend.h>
+
+static void (*pm_idle_old)(void);
+static atomic_t active_count = ATOMIC_INIT(0);
+
+struct smartass_info_s {
+	struct cpufreq_policy *cur_policy;
+	struct timer_list timer;
+	u64 time_in_idle;
+	u64 idle_exit_time;
+	int idling;
+	unsigned int force_ramp_up;
+	unsigned int enable;
+};
+static DEFINE_PER_CPU(struct smartass_info_s, smartass_info);
+
+/* Workqueues handle frequency scaling */
+static struct workqueue_struct *up_wq;
+static struct workqueue_struct *down_wq;
+static struct work_struct freq_scale_work;
+
+static u64 freq_change_time;
+static u64 freq_change_time_in_idle;
+
+static cpumask_t work_cpumask;
+static spinlock_t cpumask_lock;
+static unsigned int suspended;
+
+/*
+ * The minimum amount of time to spend at a frequency before we can ramp down,
+ * default is 45ms.
+ */
+#define DEFAULT_DOWN_RATE_US 33000;
+static unsigned long down_rate_us;
+
+/*
+ * When ramping up frequency with no idle cycles jump to at least this frequency.
+ * Zero disables. Set a very high value to jump to policy max freqeuncy.
+ */
+#define DEFAULT_UP_MIN_FREQ 1188000
+static unsigned int up_min_freq;
+
+/*
+ * When sleep_max_freq>0 the frequency when suspended will be capped
+ * by this frequency. Also will wake up at max frequency of policy
+ * to minimize wakeup issues.
+ * Set sleep_max_freq=0 to disable this behavior.
+ */
+#define DEFAULT_SLEEP_MAX_FREQ 594000
+static unsigned int sleep_max_freq;
+
+/*
+ * Sampling rate, I highly recommend to leave it at 2.
+ */
+#define DEFAULT_SAMPLE_RATE_JIFFIES 2
+static unsigned int sample_rate_jiffies;
+
+/*
+ * Freqeuncy delta when ramping up.
+ * zero disables causes to always jump straight to max frequency.
+ */
+#define DEFAULT_RAMP_UP_STEP 115200
+static unsigned int ramp_up_step;
+
+/*
+ * Max freqeuncy delta when ramping down. zero disables.
+ */
+#define DEFAULT_MAX_RAMP_DOWN 230400
+static unsigned int max_ramp_down;
+
+/*
+ * CPU freq will be increased if measured load > max_cpu_load;
+ */
+#define DEFAULT_MAX_CPU_LOAD 70
+static unsigned long max_cpu_load;
+
+/*
+ * CPU freq will be decreased if measured load < min_cpu_load;
+ */
+#define DEFAULT_MIN_CPU_LOAD 30
+static unsigned long min_cpu_load;
+
+
+static int cpufreq_governor_smartass(struct cpufreq_policy *policy,
+		unsigned int event);
+
+#ifndef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS
+static
+#endif
+struct cpufreq_governor cpufreq_gov_smartass = {
+	.name = "smartass",
+	.governor = cpufreq_governor_smartass,
+	.max_transition_latency = 9000000,
+	.owner = THIS_MODULE,
+};
+
+static void cpufreq_smartass_timer(unsigned long data)
+{
+	u64 delta_idle;
+	u64 update_time;
+	u64 now_idle;
+	unsigned long flags;
+
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, data);
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+
+	now_idle = get_cpu_idle_time_us(data, &update_time);
+
+	if (update_time == this_smartass->idle_exit_time)
+		return;
+
+	delta_idle = cputime64_sub(now_idle, this_smartass->time_in_idle);
+	//printk(KERN_INFO "smartass: t=%llu i=%llu\n",cputime64_sub(update_time,this_smartass->idle_exit_time),delta_idle);
+
+	/* Scale up if there were no idle cycles since coming out of idle */
+	if (delta_idle == 0) {
+		if (policy->cur == policy->max)
+			return;
+
+		if (nr_running() < 1)
+			return;
+
+		this_smartass->force_ramp_up = 1;
+		spin_lock_irqsave(&cpumask_lock, flags);
+		cpumask_set_cpu(data, &work_cpumask);
+		spin_unlock_irqrestore(&cpumask_lock, flags);
+		queue_work(up_wq, &freq_scale_work);
+		return;
+	}
+
+	/*
+	 * There is a window where if the cpu utlization can go from low to high
+	 * between the timer expiring, delta_idle will be > 0 and the cpu will
+	 * be 100% busy, preventing idle from running, and this timer from
+	 * firing. So setup another timer to fire to check cpu utlization.
+	 * Do not setup the timer if there is no scheduled work.
+	 */
+	if (!timer_pending(&this_smartass->timer) && nr_running() > 0) { 
+			this_smartass->time_in_idle = get_cpu_idle_time_us(
+					data, &this_smartass->idle_exit_time);
+			mod_timer(&this_smartass->timer, jiffies + sample_rate_jiffies);
+	}
+
+	if (policy->cur == policy->min)
+		return;
+
+	/*
+	 * Do not scale down unless we have been at this frequency for the
+	 * minimum sample time.
+	 */
+	if (cputime64_sub(update_time, freq_change_time) < down_rate_us)
+		return;
+
+	spin_lock_irqsave(&cpumask_lock, flags);
+	cpumask_set_cpu(data, &work_cpumask);
+	spin_unlock_irqrestore(&cpumask_lock, flags);
+	queue_work(down_wq, &freq_scale_work);
+}
+
+static void cpufreq_idle(void)
+{
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, smp_processor_id());
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+	int pending;
+	unsigned long flags;
+
+	if (!this_smartass->enable) {
+		pm_idle_old();
+		return;
+	}
+
+	this_smartass->idling = 1;
+	smp_wmb();
+	pending = timer_pending(&this_smartass->timer);
+
+	if (this_smartass->cur_policy->cur != this_smartass->cur_policy->min) {
+#ifdef CONFIG_SMP
+		/*
+		 * Entering idle while not at lowest speed.  On some
+		 * platforms this can hold the other CPU(s) at that speed
+		 * even though the CPU is idle. Set a timer to re-evaluate
+		 * speed so this idle CPU doesn't hold the other CPUs above
+		 * min indefinitely.  This should probably be a quirk of
+		 * the CPUFreq driver.
+		 */
+		if (!pending) {
+			this_smartass->time_in_idle = get_cpu_idle_time_us(
+				smp_processor_id(), &this_smartass->idle_exit_time);
+			mod_timer(&this_smartass->timer, jiffies + 2);
+		}
+#endif
+	}
+
+	pm_idle_old();
+	this_smartass->idling = 0;
+	smp_wmb();
+
+	spin_lock_irqsave(&cpumask_lock, flags);
+	if (!cpumask_test_cpu(smp_processor_id(), policy->cpus)) {
+		spin_unlock_irqrestore(&cpumask_lock, flags);
+		return;
+	}
+	spin_unlock_irqrestore(&cpumask_lock, flags);
+
+	/* Timer to fire in 1-2 ticks, jiffie aligned. */
+	if (timer_pending(&this_smartass->timer) == 0) {
+		this_smartass->time_in_idle = get_cpu_idle_time_us(
+				smp_processor_id(), &this_smartass->idle_exit_time);
+		mod_timer(&this_smartass->timer, jiffies + sample_rate_jiffies);
+	}
+}
+
+/*
+ * Choose the cpu frequency based off the load. For now choose the minimum
+ * frequency that will satisfy the load, which is not always the lower power.
+ */
+static unsigned int cpufreq_smartass_calc_freq(unsigned int cpu, struct cpufreq_policy *policy)
+{
+	unsigned int delta_time;
+	unsigned int idle_time;
+	unsigned int cpu_load;
+	unsigned int new_freq;
+	u64 current_wall_time;
+	u64 current_idle_time;
+
+	current_idle_time = get_cpu_idle_time_us(cpu, &current_wall_time);
+
+	idle_time = (unsigned int)( current_idle_time - freq_change_time_in_idle );
+	delta_time = (unsigned int)( current_wall_time - freq_change_time );
+
+	cpu_load = 100 * (delta_time - idle_time) / delta_time;
+	//printk(KERN_INFO "Smartass calc_freq: delta_time=%u cpu_load=%u\n",delta_time,cpu_load);
+	if (cpu_load < min_cpu_load) {
+		cpu_load += 100 - max_cpu_load; // dummy load.
+		new_freq = policy->cur * cpu_load / 100;
+		if (max_ramp_down && new_freq < policy->cur - max_ramp_down)
+			new_freq = policy->cur - max_ramp_down;
+		//printk(KERN_INFO "Smartass calc_freq: %u => %u\n",policy->cur,new_freq);
+		return new_freq;
+	} if (cpu_load > max_cpu_load) {
+		if (ramp_up_step)
+			new_freq = policy->cur + ramp_up_step;
+		else
+			new_freq = policy->max;
+		return new_freq;
+	}
+	return policy->cur;
+}
+
+/* We use the same work function to sale up and down */
+static void cpufreq_smartass_freq_change_time_work(struct work_struct *work)
+{
+	unsigned int cpu;
+	unsigned int new_freq;
+	struct smartass_info_s *this_smartass;
+	struct cpufreq_policy *policy;
+	unsigned long flags;
+	cpumask_t tmp_mask;
+
+	spin_lock_irqsave(&cpumask_lock, flags);
+	tmp_mask = work_cpumask;
+	spin_unlock_irqrestore(&cpumask_lock, flags);
+
+	for_each_cpu(cpu, &tmp_mask) {
+		this_smartass = &per_cpu(smartass_info, cpu);
+		policy = this_smartass->cur_policy;
+
+		if (this_smartass->force_ramp_up) {
+			this_smartass->force_ramp_up = 0;
+
+			if (nr_running() == 1) {
+				spin_lock_irqsave(&cpumask_lock, flags);
+				cpumask_clear_cpu(cpu, &work_cpumask);
+				spin_unlock_irqrestore(&cpumask_lock, flags);
+				return;
+			}
+
+			if (policy->cur == policy->max)
+				return;
+
+			if (ramp_up_step)
+				new_freq = policy->cur + ramp_up_step;
+			else
+				new_freq = policy->max;
+
+			if (suspended && sleep_max_freq) {
+				if (new_freq > sleep_max_freq)
+					new_freq = sleep_max_freq;
+			} else {
+				if (new_freq < up_min_freq)
+					new_freq = up_min_freq;
+			}
+
+		} else {
+			new_freq = cpufreq_smartass_calc_freq(cpu,policy);
+
+			// in suspend limit to sleep_max_freq and
+			// jump straight to sleep_max_freq to avoid wakeup problems
+			if (suspended && sleep_max_freq &&
+			    (new_freq > sleep_max_freq || new_freq > policy->cur))
+				new_freq = sleep_max_freq;
+		}
+
+		if (new_freq > policy->max)
+			new_freq = policy->max;
+		
+		if (new_freq < policy->min)
+			new_freq = policy->min;
+		
+		__cpufreq_driver_target(policy, new_freq,
+					CPUFREQ_RELATION_L);
+
+		spin_lock_irqsave(&cpumask_lock, flags);
+		freq_change_time_in_idle = get_cpu_idle_time_us(cpu,
+							&freq_change_time);
+
+		cpumask_clear_cpu(cpu, &work_cpumask);
+		spin_unlock_irqrestore(&cpumask_lock, flags);
+	}
+
+
+}
+
+static ssize_t show_down_rate_us(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf(buf, "%lu\n", down_rate_us);
+}
+
+static ssize_t store_down_rate_us(struct cpufreq_policy *policy, const char *buf, size_t count)
+{
+        ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 1000 && input <= 100000000)
+	  down_rate_us = input;
+	return res;
+}
+
+static struct freq_attr down_rate_us_attr = __ATTR(down_rate_us, 0644,
+		show_down_rate_us, store_down_rate_us);
+
+static ssize_t show_up_min_freq(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf(buf, "%u\n", up_min_freq);
+}
+
+static ssize_t store_up_min_freq(struct cpufreq_policy *policy, const char *buf, size_t count)
+{
+        ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+	  up_min_freq = input;
+	return res;
+}
+
+static struct freq_attr up_min_freq_attr = __ATTR(up_min_freq, 0644,
+		show_up_min_freq, store_up_min_freq);
+
+static ssize_t show_sleep_max_freq(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf(buf, "%u\n", sleep_max_freq);
+}
+
+static ssize_t store_sleep_max_freq(struct cpufreq_policy *policy, const char *buf, size_t count)
+{
+        ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input >= 0)
+	  sleep_max_freq = input;
+	return res;
+}
+
+static struct freq_attr sleep_max_freq_attr = __ATTR(sleep_max_freq, 0644,
+		show_sleep_max_freq, store_sleep_max_freq);
+
+static ssize_t show_sample_rate_jiffies(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf(buf, "%u\n", sample_rate_jiffies);
+}
+
+static ssize_t store_sample_rate_jiffies(struct cpufreq_policy *policy, const char *buf, size_t count)
+{
+        ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 1000)
+	  sample_rate_jiffies = input;
+	return res;
+}
+
+static struct freq_attr sample_rate_jiffies_attr = __ATTR(sample_rate_jiffies, 0644,
+		show_sample_rate_jiffies, store_sample_rate_jiffies);
+
+static ssize_t show_ramp_up_step(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf(buf, "%u\n", ramp_up_step);
+}
+
+static ssize_t store_ramp_up_step(struct cpufreq_policy *policy, const char *buf, size_t count)
+{
+        ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0)
+	  ramp_up_step = input;
+	return res;
+}
+
+static struct freq_attr ramp_up_step_attr = __ATTR(ramp_up_step, 0644,
+		show_ramp_up_step, store_ramp_up_step);
+
+static ssize_t show_max_ramp_down(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf(buf, "%u\n", max_ramp_down);
+}
+
+static ssize_t store_max_ramp_down(struct cpufreq_policy *policy, const char *buf, size_t count)
+{
+        ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0)
+	  max_ramp_down = input;
+	return res;
+}
+
+static struct freq_attr max_ramp_down_attr = __ATTR(max_ramp_down, 0644,
+		show_max_ramp_down, store_max_ramp_down);
+
+static ssize_t show_max_cpu_load(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf(buf, "%lu\n", max_cpu_load);
+}
+
+static ssize_t store_max_cpu_load(struct cpufreq_policy *policy, const char *buf, size_t count)
+{
+        ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input <= 100)
+	  max_cpu_load = input;
+	return res;
+}
+
+static struct freq_attr max_cpu_load_attr = __ATTR(max_cpu_load, 0644,
+		show_max_cpu_load, store_max_cpu_load);
+
+static ssize_t show_min_cpu_load(struct cpufreq_policy *policy, char *buf)
+{
+	return sprintf(buf, "%lu\n", min_cpu_load);
+}
+
+static ssize_t store_min_cpu_load(struct cpufreq_policy *policy, const char *buf, size_t count)
+{
+        ssize_t res;
+	unsigned long input;
+	res = strict_strtoul(buf, 0, &input);
+	if (res >= 0 && input > 0 && input < 100)
+	  min_cpu_load = input;
+	return res;
+}
+
+static struct freq_attr min_cpu_load_attr = __ATTR(min_cpu_load, 0644,
+		show_min_cpu_load, store_min_cpu_load);
+
+static struct attribute * smartass_attributes[] = {
+	&down_rate_us_attr.attr,
+	&up_min_freq_attr.attr,
+	&sleep_max_freq_attr.attr,
+	&sample_rate_jiffies_attr.attr,
+	&ramp_up_step_attr.attr,
+	&max_ramp_down_attr.attr,
+	&max_cpu_load_attr.attr,
+	&min_cpu_load_attr.attr,
+	NULL,
+};
+
+static struct attribute_group smartass_attr_group = {
+	.attrs = smartass_attributes,
+	.name = "smartass",
+};
+
+static int cpufreq_governor_smartass(struct cpufreq_policy *new_policy,
+		unsigned int event)
+{
+	unsigned int cpu = new_policy->cpu;
+	int rc;
+
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, cpu);
+	
+	switch (event) {
+	case CPUFREQ_GOV_START:
+		if ((!cpu_online(cpu)) || (!new_policy->cur))
+			return -EINVAL;
+
+		if (this_smartass->enable) /* Already enabled */
+			break;
+
+		/*
+		 * Do not register the idle hook and create sysfs
+		 * entries if we have already done so.
+		 */
+		if (atomic_inc_return(&active_count) > 1)
+			return 0;
+
+		rc = sysfs_create_group(&new_policy->kobj, &smartass_attr_group);
+		if (rc)
+			return rc;
+		pm_idle_old = pm_idle;
+		pm_idle = cpufreq_idle;
+
+		this_smartass->cur_policy = new_policy;
+		this_smartass->cur_policy->max = CONFIG_MSM_CPU_FREQ_ONDEMAND_MAX;
+		this_smartass->cur_policy->min = CONFIG_MSM_CPU_FREQ_ONDEMAND_MIN;
+		this_smartass->cur_policy->cur = CONFIG_MSM_CPU_FREQ_ONDEMAND_MAX;
+		this_smartass->enable = 1;
+
+		// notice no break here!
+
+	case CPUFREQ_GOV_LIMITS:
+		if (this_smartass->cur_policy->cur != new_policy->max) {
+			__cpufreq_driver_target(new_policy, new_policy->max, CPUFREQ_RELATION_H);
+		}
+		break;
+
+	case CPUFREQ_GOV_STOP:
+		this_smartass->enable = 0;
+
+		if (atomic_dec_return(&active_count) > 1)
+			return 0;
+		sysfs_remove_group(&new_policy->kobj,
+				&smartass_attr_group);
+
+		pm_idle = pm_idle_old;
+		del_timer(&this_smartass->timer);
+		break;
+	}
+
+	return 0;
+}
+
+static void smartass_suspend(int cpu, int suspend)
+{
+	struct smartass_info_s *this_smartass = &per_cpu(smartass_info, smp_processor_id());
+	struct cpufreq_policy *policy = this_smartass->cur_policy;
+	unsigned int new_freq;
+
+	if (!this_smartass->enable || sleep_max_freq==0) // disable behavior for sleep_max_freq==0
+		return;
+
+	if (suspend) {
+	    if (policy->cur > sleep_max_freq) {
+			new_freq = sleep_max_freq;
+			if (new_freq > policy->max)
+				new_freq = policy->max;
+			if (new_freq < policy->min)
+				new_freq = policy->min;
+			__cpufreq_driver_target(policy, new_freq,
+						CPUFREQ_RELATION_H);
+		}
+	} else { // resume at max speed:
+		__cpufreq_driver_target(policy, policy->max,
+					CPUFREQ_RELATION_H);
+	}
+
+}
+
+static void smartass_early_suspend(struct early_suspend *handler) {
+	int i;
+	suspended = 1;
+	for_each_online_cpu(i)
+		smartass_suspend(i,1);
+}
+
+static void smartass_late_resume(struct early_suspend *handler) {
+	int i;
+	suspended = 0;
+	for_each_online_cpu(i)
+		smartass_suspend(i,0);
+}
+
+static struct early_suspend smartass_power_suspend = {
+	.suspend = smartass_early_suspend,
+	.resume = smartass_late_resume,
+};
+
+static int __init cpufreq_smartass_init(void)
+{	
+	unsigned int i;
+	struct smartass_info_s *this_smartass;
+	down_rate_us = DEFAULT_DOWN_RATE_US;
+	up_min_freq = DEFAULT_UP_MIN_FREQ;
+	sleep_max_freq = DEFAULT_SLEEP_MAX_FREQ;
+	sample_rate_jiffies = DEFAULT_SAMPLE_RATE_JIFFIES;
+	ramp_up_step = DEFAULT_RAMP_UP_STEP;
+	max_ramp_down = DEFAULT_MAX_RAMP_DOWN;
+	max_cpu_load = DEFAULT_MAX_CPU_LOAD;
+	min_cpu_load = DEFAULT_MIN_CPU_LOAD;
+
+	suspended = 0;
+
+	/* Initalize per-cpu data: */
+	for_each_possible_cpu(i) {
+		this_smartass = &per_cpu(smartass_info, i);
+		this_smartass->enable = 0;
+		this_smartass->force_ramp_up = 0;
+		this_smartass->time_in_idle = 0;
+		this_smartass->idle_exit_time = 0;
+		// intialize timer:
+		init_timer_deferrable(&this_smartass->timer);
+		this_smartass->timer.function = cpufreq_smartass_timer;
+		this_smartass->timer.data = i;
+	}
+
+	/* Scale up is high priority */
+	up_wq = create_rt_workqueue("ksmartass_up");
+	down_wq = create_workqueue("ksmartass_down");
+
+	INIT_WORK(&freq_scale_work, cpufreq_smartass_freq_change_time_work);
+
+	spin_lock_init(&cpumask_lock);
+
+	register_early_suspend(&smartass_power_suspend);
+
+	return cpufreq_register_governor(&cpufreq_gov_smartass);
+}
+
+#ifdef CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS
+pure_initcall(cpufreq_smartass_init);
+#else
+module_init(cpufreq_smartass_init);
+#endif
+
+static void __exit cpufreq_smartass_exit(void)
+{
+	cpufreq_unregister_governor(&cpufreq_gov_smartass);
+	destroy_workqueue(up_wq);
+	destroy_workqueue(down_wq);
+}
+
+module_exit(cpufreq_smartass_exit);
+
+MODULE_AUTHOR ("Erasmux");
+MODULE_DESCRIPTION ("'cpufreq_smartass' - A smart cpufreq governor");
+MODULE_LICENSE ("GPL");
diff --git a/include/linux/cpufreq.h b/include/linux/cpufreq.h
index e71e0f6..56e72a3 100644
--- a/include/linux/cpufreq.h
+++ b/include/linux/cpufreq.h
@@ -364,6 +364,12 @@ extern struct cpufreq_governor cpufreq_gov_ondemand;
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_CONSERVATIVE)
 extern struct cpufreq_governor cpufreq_gov_conservative;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_conservative)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_MINMAX)
+extern struct cpufreq_governor cpufreq_gov_minmax;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_minmax)
+#elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_SMARTASS)
+extern struct cpufreq_governor cpufreq_gov_smartass;
+#define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_smartass)
 #elif defined(CONFIG_CPU_FREQ_DEFAULT_GOV_INTERACTIVE)
 extern struct cpufreq_governor cpufreq_gov_interactive;
 #define CPUFREQ_DEFAULT_GOVERNOR	(&cpufreq_gov_interactive)
-- 
1.7.4.1

